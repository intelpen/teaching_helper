{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import dotenv\n",
    "import time\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = 'clasificationfromdescription'\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = 'us-central1'\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "404 NOT_FOUND. {'error': {'code': 404, 'message': 'Publisher Model `projects/clasificationfromdescription/locations/us-central1/publishers/google/models/gemini-1` not found.', 'status': 'NOT_FOUND'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     26\u001b[0m backend_llm \u001b[38;5;241m=\u001b[39m LLMGemini2()\n\u001b[0;32m---> 27\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhy is six afraid of seven ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Example response:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Here's a simplified overview:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mLLMGemini2.process_request\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, message):\n\u001b[0;32m---> 16\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     response_rag  \u001b[38;5;241m=\u001b[39m RagResponse()\n\u001b[1;32m     21\u001b[0m     response_rag\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/miniconda3/envs/py311vertex/lib/python3.11/site-packages/google/genai/models.py:4672\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4670\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4671\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4672\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4673\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m   4674\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4675\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   4676\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311vertex/lib/python3.11/site-packages/google/genai/models.py:3831\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3828\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   3829\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 3831\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[1;32m   3833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[1;32m   3836\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[1;32m   3837\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[1;32m   3838\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/py311vertex/lib/python3.11/site-packages/google/genai/_api_client.py:455\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    447\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m     http_options: HttpOptionsOrDict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    451\u001b[0m ):\n\u001b[1;32m    452\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    453\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m    454\u001b[0m   )\n\u001b[0;32m--> 455\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[1;32m    457\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[0;32m~/miniconda3/envs/py311vertex/lib/python3.11/site-packages/google/genai/_api_client.py:385\u001b[0m, in \u001b[0;36mApiClient._request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    377\u001b[0m   authed_session\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m stream\n\u001b[1;32m    378\u001b[0m   response \u001b[38;5;241m=\u001b[39m authed_session\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    379\u001b[0m       http_request\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    380\u001b[0m       http_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    384\u001b[0m   )\n\u001b[0;32m--> 385\u001b[0m   \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    387\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    388\u001b[0m   )\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/py311vertex/lib/python3.11/site-packages/google/genai/errors.py:100\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m     98\u001b[0m status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[1;32m    102\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response)\n",
      "\u001b[0;31mClientError\u001b[0m: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'Publisher Model `projects/clasificationfromdescription/locations/us-central1/publishers/google/models/gemini-1` not found.', 'status': 'NOT_FOUND'}}"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai.types import HttpOptions\n",
    "\n",
    "from backend.llms.llm_rag import LLMProtocol\n",
    "\n",
    "class RagResponse():\n",
    "    def __init__(self, text = None):\n",
    "        self.text = text\n",
    "model=\"gemini-1.0-flash\"\n",
    "class LLMGemini2(LLMProtocol):\n",
    "    def __init__(self, config=None):\n",
    "        self.client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    " \n",
    "    def process_request(self, message):\n",
    "       \n",
    "        response = self.client.models.generate_content(\n",
    "            model=model,\n",
    "            contents=message,\n",
    "        )\n",
    "        response_rag  = RagResponse()\n",
    "        response_rag.text = response.text\n",
    "        response_rag.raw_response = response\n",
    "        return response_rag\n",
    "    def log(self ):\n",
    "        pass\n",
    "backend_llm = LLMGemini2()\n",
    "response = backend_llm.process_request(\"why is six afraid of seven ?\")\n",
    "# Example response:\n",
    "# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...\n",
    "#\n",
    "# Here's a simplified overview:\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a classic kid\\'s joke!  The answer is:\\n\\nBecause **seven ate nine**. \\n\\nIt plays on the sound of \"ate\" and \"eight\".  😊 \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config {'chunker_name': 'chunks', 'similarity_threshold': 0.2, 'top_k': 5, 'no_internal_source_prompt': 'Your only answer should be exactly like this: Nu am gasit informatii in curs despre intrebarea ta', 'internal_source_rules': 'Esti un asistent care raspunde in limba Romana la intrebarea din  \"ORIGINAL USER PROMPT\" folosind context din \"CONTEXT FROM INTERNAL SOURCES\". Raspunde folosind numai informatii din context. Raspunsul tau trebuie sa fie complet si sa contina cat mai mult din contextul intrebarii. Vei raspunde \\'Folosind informatiile din curs, \\' si apoi vei raspunde'} \n",
      " -------------------------------\n",
      "No embeddings db directory found!\n",
      "Loading documents..\n",
      "Splitting by chunks\n",
      "\n",
      "loading documents..\n",
      "Current directory: /data/ai/dirigu/teaching_helper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:03<00:00,  3.82it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_rag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMRag\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#from backend.llms.llm_gemini import LLMGemini\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m llmrag \u001b[38;5;241m=\u001b[39m \u001b[43mLLMRag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m llmrag\u001b[38;5;241m.\u001b[39mprocess_request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCe e un subquery  ?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/data/ai/dirigu/teaching_helper/backend/llms/llm_rag.py:33\u001b[0m, in \u001b[0;36mLLMRag.__init__\u001b[0;34m(self, config, backend)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m=\u001b[39m backend\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# initialize chroma if needded\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# self.config = config[self.backend.name]\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39membedding_function\n",
      "File \u001b[0;32m/data/ai/dirigu/teaching_helper/backend/llms/vector_store.py:39\u001b[0m, in \u001b[0;36mVectorStoreWrapper.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSplitting by chunks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_default_docs_and_split_into_chunks()\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocuments len \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSplitting by pages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# Load my LLM \n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../..')\n",
    "\n",
    "\n",
    "from backend.llms.llm_rag import LLMRag\n",
    "#from backend.llms.llm_gemini import LLMGemini\n",
    "\n",
    "\n",
    "llmrag = LLMRag(backend=backend_llm)\n",
    "response = llmrag.process_request('Ce e un subquery  ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "intrebari_df = pd.read_csv(\"ragas_results_G1.5.csv\")[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrebari_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_queries = [intrebari_dict[x]['query'] for x in intrebari_df]\n",
    "# expected_responses = [intrebari_dict[x]['expected_response'] for x in intrebari_dict]\n",
    "sample_queries = intrebari_df[\"user_input\"].values\n",
    "\n",
    "expected_responses = intrebari_df[\"reference\"].values\n",
    "expected_responses = [x.removeprefix(\"Folosind informatiile din curs\") for x in expected_responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "i=0\n",
    "for query,reference in zip(sample_queries,expected_responses):\n",
    "    print(f'question{i}')\n",
    "    i+=1\n",
    "    full_response = llmrag.process_request(query)\n",
    "    time.sleep(10)\n",
    "    relevant_docs = full_response.context\n",
    "    response = full_response.text.removeprefix(\"Folosind informatiile din curs, \")\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\":query,\n",
    "            \"retrieved_contexts\":relevant_docs,\n",
    "            \"response\":response,\n",
    "            \"reference\":reference\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "from ragas.metrics import ContextPrecision,  LLMContextRecall, AnswerRelevancy,  Faithfulness,  BleuScore, RougeScore, FactualCorrectness\n",
    "metrics = [\n",
    "        ContextPrecision(),\n",
    "        LLMContextRecall(),\n",
    "        AnswerRelevancy(),\n",
    "        Faithfulness(),\n",
    "        BleuScore(),\n",
    "        RougeScore(),\n",
    "        FactualCorrectness()]\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=evaluator_llm)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = \"apt.4418-2f131daa8aa7-086e-8dd7-0123d22b-08626\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ragas_results_{model}_{llmrag.config['chunker_name']}.csv\")\n",
    "result.to_pandas().to_csv(f\"ragas_results_{model}_{llmrag.config['chunker_name']}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.upload()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311vertex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
